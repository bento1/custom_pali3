{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#현재 폴더 경로; 작업 폴더 기준\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/content/drive/MyDrive/chart2text/PALI3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomPALI3.SummaryChartDataset import SummaryChartDataset\n",
    "from CustomPALI3.CustomPALI3 import CustomPALI3Config,CustomPALI3\n",
    "from transformers import T5Tokenizer\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from pytorch_model_summary import summary\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "dataset_repo=[{'dataset':'timm/imagenet-12k-wds','config':'default','type':'vision-text'},\n",
    "                {'dataset':'wikimedia/wikipedia','config':'20231101.en','type':'text'},\n",
    "                {'dataset':'conceptual_captions','config':'labeled','type':'vision-text'},\n",
    "                {'dataset':'poloclub/diffusiondb','config':'2m_random_1m','type':'vision-text'},\n",
    "                ]\n",
    "\n",
    "def pretrain(\n",
    "    model,\n",
    "    args,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    \n",
    "):\n",
    "    model_path=args['output_dir']\n",
    "    scaler = GradScaler()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(int(args['num_epochs'])):\n",
    "        model.model.train()\n",
    "        step_num=0\n",
    "        for _ in range(int(args['max_steps'])):\n",
    "            try:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                input_data = next(iter(train_loader))\n",
    "                images=input_data['image']\n",
    "                input_ids=input_data['input_ids']\n",
    "                attn_masks=input_data['attn_mask']\n",
    "                # print(len(input_ids))\n",
    "                for sub_step in range(len(input_ids)):\n",
    "                    # print(model.pali_model.parameters())\n",
    "                    image = images[sub_step].to(device)\n",
    "                    prompt=input_ids[sub_step].to(device)\n",
    "                    output=input_ids[sub_step].to(device)\n",
    "                    attn_mask=attn_masks[sub_step].to(device)\n",
    "                    # print(images[sub_step].shape,input_ids[sub_step].shape,attn_masks[sub_step].shape)\n",
    "                    optimizer.zero_grad()\n",
    "                    prev_dec_some_weight=model.model.pali_model.decoder.net.attn_layers.layers[0][1].to_out.weight[0,0].item()\n",
    "                    prev_enc_some_weight=model.model.pali_model.encoder.attn_layers.layers[1][1].ff[0][0].weight[0,0].item()\n",
    "                    prev_vit_some_weight=model.model.vit_model.model.model.vision_model.encoder.layers[-1].mlp.fc1.weight[0,0].item()\n",
    "                    with autocast(dtype=torch.bfloat16):\n",
    "                        logits, loss = model(img=image,prompt=prompt,output=output,mask=attn_mask)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    # logits, loss = model(img=image,prompt=prompt,output=output,mask=attn_mask)\n",
    "                    # loss.backward()\n",
    "                    # optimizer.step()\n",
    "                    # scheduler.step()\n",
    "                    prev_dec_some_weight2=model.model.pali_model.decoder.net.attn_layers.layers[0][1].to_out.weight[0,0].item()\n",
    "                    prev_enc_some_weight2=model.model.pali_model.encoder.attn_layers.layers[1][1].ff[0][0].weight[0,0].item()\n",
    "                    prev_vit_some_weight2=model.model.vit_model.model.model.vision_model.encoder.layers[-1].mlp.fc1.weight[0,0].item()\n",
    "                    print('diff_vit_enc:',prev_vit_some_weight2-prev_vit_some_weight,'diff_text_enc:',prev_enc_some_weight2-prev_enc_some_weight,'diff_text_dec',prev_dec_some_weight2-prev_dec_some_weight)\n",
    "                    print(f\"Epoch: {epoch+1}, Step: {step_num+1}, Train Loss: {loss}\")\n",
    "                    step_num+=1\n",
    "                scheduler.step()\n",
    "                if step_num%100==0 and step_num!=0:\n",
    "                    save_checkpoint(model,model_path+'_temp')\n",
    "            except Exception as e:\n",
    "                print('occurs error : ',e)\n",
    "                continue\n",
    "\n",
    "        val_loss = validate(model, val_loader, device,args)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Train Loss: {loss}, Val Loss: {val_loss}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(model,model_path)\n",
    "\n",
    "def validate(model, dataloader, device,args):\n",
    "    model.model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(int(args['valid_steps'])):\n",
    "            try:\n",
    "                input_data = next(iter(dataloader))\n",
    "                images=input_data['image']\n",
    "                input_ids=input_data['input_ids']\n",
    "                attn_masks=input_data['attn_mask']\n",
    "                for sub_step in range(len(input_ids)):\n",
    "                    image = images[sub_step].to(device)\n",
    "                    prompt=input_ids[sub_step].to(device)\n",
    "                    output=input_ids[sub_step].to(device)\n",
    "                    attn_mask=attn_masks[sub_step].to(device)\n",
    "                    logits, loss = model(img=image,prompt=prompt,output=output,mask=attn_mask)\n",
    "                    total_loss += loss\n",
    "            except:\n",
    "                continue\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def save_checkpoint(model,save_path):\n",
    "    model.save_pretrained(save_path, from_pt=True)\n",
    "\n",
    "def my_collate_fn(samples):\n",
    "    image_batch = []\n",
    "    input_batch = []\n",
    "    attn_mask_batch = []\n",
    "    \n",
    "    batch_size=len(samples)\n",
    "    \n",
    "    image_batch_ = []\n",
    "    input_batch_ = []\n",
    "    attn_mask_batch_ = []\n",
    "    for sample in samples:\n",
    "        image_batch_.extend(sample['image'])\n",
    "        input_batch_.extend(sample['input_ids'])\n",
    "        attn_mask_batch_.extend(sample['attn_mask'])\n",
    "    \n",
    "    total_b=len(image_batch_)//batch_size # 14 //4   3.xx 3  \n",
    "    total_b=total_b+1 if len(image_batch_)%batch_size!=0  else total_b\n",
    "    for i in range(total_b):\n",
    "        if (i+1)*batch_size<len(image_batch_):\n",
    "            image_batch.append(torch.stack(image_batch_[i*batch_size:(i+1)*batch_size]))\n",
    "            input_batch.append(torch.stack(input_batch_[i*batch_size:(i+1)*batch_size]))\n",
    "            attn_mask_batch.append(torch.stack(attn_mask_batch_[i*batch_size:(i+1)*batch_size]))\n",
    "        else:\n",
    "            image_batch.append(torch.stack(image_batch_[i*batch_size:]))\n",
    "            input_batch.append(torch.stack(input_batch_[i*batch_size:]))\n",
    "            attn_mask_batch.append(torch.stack(attn_mask_batch_[i*batch_size:]))\n",
    "\n",
    "    return {'image': image_batch, 'input_ids': input_batch,'attn_mask':attn_mask_batch}\n",
    "def getParameters(model):\n",
    "    count=0\n",
    "    def mul(list_):\n",
    "            init=1\n",
    "            for i in list_:\n",
    "                    init*=i\n",
    "            return init\n",
    "    for name, param in model.vit_model.named_parameters():\n",
    "            count+=mul(np.array(param.size()).tolist())\n",
    "    print('vit_model',count/1000000,\"M\")\n",
    "    count=0\n",
    "    for name, param in model.pali_model.named_parameters():\n",
    "            count+=mul(np.array(param.size()).tolist())\n",
    "    print('pali_model',count/1000000,\"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "    args={\n",
    "        'output_dir':'/content/drive/MyDrive/chart2text/PALI3/output',\n",
    "        'lr':1e-4,\n",
    "        'max_steps':1e4,\n",
    "        'valid_steps':1e2,\n",
    "        'num_epochs':100,\n",
    "        'batch_size':8,\n",
    "        'num_training_samples_per_epoch':10,\n",
    "        'max_epochs':100,\n",
    "        \"warmup_steps\":100,\n",
    "        'num_workers':1,\n",
    "        'num_nodes':1,\n",
    "        }\n",
    "    \n",
    "    tokenizer=T5Tokenizer.from_pretrained(\"google/flan-t5-base\", bos_token = '<s>',add_bos_token = True)\n",
    "    train_loader=DataLoader(SummaryChartDataset(dataset_repo,1024,tokenizer,'</s>','train'), batch_size=1, shuffle=True, num_workers=1,collate_fn=my_collate_fn)\n",
    "    val_loader=DataLoader(SummaryChartDataset(dataset_repo,1024,tokenizer,'</s>','validation'), batch_size=1, shuffle=True, num_workers=1,collate_fn=my_collate_fn)\n",
    "    config=CustomPALI3Config(version=1,model_name='test',\n",
    "                        dim=1024,enc_num_tokens=32100,enc_max_seq_len=1024,\n",
    "                        dec_num_tokens=32100,dec_max_seq_len=1024,enc_depth=12,enc_heads=16,dec_depth=12,dec_heads=16,seq_len=1024\n",
    "                        ,device='mps',vit_fix=False)\n",
    "    \n",
    "    # device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model=CustomPALI3(config)\n",
    "    model=model.from_pretrained(\"/Users/dongunyun/study/datascience/chart2text/PALI3/output_temp\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    scheduler = StepLR(optimizer, step_size=500, gamma=0.1)\n",
    "\n",
    "    summary(model,torch.zeros((1,3,336,336)).to(device=device,dtype=torch.long),\n",
    "                            torch.zeros((1,1024)).to(device=device,dtype=torch.int),\n",
    "                            torch.zeros((1,1024)).to(device=device,dtype=torch.int),\n",
    "                            torch.ones(1, 1024).bool().to(device=device),\n",
    "                            show_input=True, print_summary=True,)\n",
    "    getParameters(model.model)\n",
    "    pretrain(\n",
    "        model,\n",
    "        args,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
