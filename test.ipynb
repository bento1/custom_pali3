{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from CustomPALI3.CustomPALI3 import CustomPALI3,CustomPALI3Config\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "img=Image.open('test.png').convert('RGB').resize((512,512))\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "device=torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "# device=torch.device(\"cpu\")\n",
    "img = torch.from_numpy(np.array(img)).permute(2, 0, 1).unsqueeze(dim=0).to(device)\n",
    "query=\"summarize this chart\"\n",
    "tonkens = tokenizer(\n",
    "        query, max_length=512, padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n",
    "prompt=tonkens['input_ids'].to(device)\n",
    "mask = tonkens['attention_mask'].numpy()\n",
    "mask=mask==1\n",
    "mask=torch.tensor(mask).to(device)\n",
    "answer=\"This trace chart displays the \\u770c's values over time, with green indicating normal values and light green indicating abnormal ones. The overshoot of the abnormal signal is approximately 57% longer than that of the normal signals. Moreover, the two signals are easily distinguishable. The amplitude of the abnormal signal is smaller than that of the normal signal.\"\n",
    "output_text = tokenizer(\n",
    "        answer, max_length=1024, padding=\"max_length\", truncation=True,return_tensors=\"pt\")['input_ids'].to(device)\n",
    "\n",
    "output_text = torch.randint(0, 256, (1, 1024)).to(device)\n",
    "config=CustomPALI3Config(model_name='test',\n",
    "        image_size=512,\n",
    "        patch_size=256,\n",
    "        dim=512,\n",
    "        depth=12,\n",
    "        heads=32,\n",
    "        enc_num_tokens=32100,\n",
    "        enc_max_seq_len=1024,\n",
    "        dec_num_tokens=32100,\n",
    "        dec_max_seq_len=1024,\n",
    "        enc_depth=12,\n",
    "        enc_heads=32,\n",
    "        dec_depth=12,\n",
    "        dec_heads=32,\n",
    "        seq_len=1024,\n",
    "        device=device)\n",
    "model = CustomPALI3(config)\n",
    "\n",
    "result=model(img, prompt, output_text, mask)\n",
    "\n",
    "# count=0\n",
    "# def mul(list_):\n",
    "#         init=1\n",
    "#         for i in list_:\n",
    "#                 init*=i\n",
    "#         return init\n",
    "# for name, param in model.vit_model.vit.named_parameters():\n",
    "#         count+=mul(np.array(param.size()).tolist())\n",
    "# print(count/1000000,\"M\")\n",
    "# count=0\n",
    "# for name, param in model.pali_model.named_parameters():\n",
    "#         count+=mul(np.array(param.size()).tolist())\n",
    "# print(count/1000000,\"M\")\n",
    "# # https://huggingface.co/datasets/timm/imagenet-22k-wds\n",
    "# # https://huggingface.co/datasets/conceptual_captions\n",
    "# # https://huggingface.co/datasets/poloclub/diffusiondb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dongunyun/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/dongunyun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-03-03 17:47:03,787 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-03-03 17:47:04,030 - DEBUG - https://huggingface.co:443 \"HEAD /google/flan-t5-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-03-03 17:47:04,201 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:04,492 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /is-valid?dataset=timm/imagenet-12k-wds&config=default HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:04,501 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:04,810 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /size?dataset=timm/imagenet-12k-wds&config=default HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:04,817 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:05,107 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /is-valid?dataset=wikimedia/wikipedia&config=20231101.en HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:05,119 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:05,450 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /size?dataset=wikimedia/wikipedia&config=20231101.en HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:05,463 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:05,859 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /is-valid?dataset=conceptual_captions&config=labeled HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:05,871 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:06,268 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /size?dataset=conceptual_captions&config=labeled HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:06,278 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:06,680 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /is-valid?dataset=poloclub/diffusiondb&config=2m_random_1m HTTP/1.1\" 500 None\n",
      "2024-03-03 17:47:06,695 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:07,087 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /is-valid?dataset=timm/imagenet-12k-wds&config=default HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:07,095 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:07,496 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /size?dataset=timm/imagenet-12k-wds&config=default HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:07,501 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:07,803 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /is-valid?dataset=wikimedia/wikipedia&config=20231101.en HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:07,807 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:08,213 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /size?dataset=wikimedia/wikipedia&config=20231101.en HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:08,217 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:08,520 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /is-valid?dataset=conceptual_captions&config=labeled HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:08,523 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:08,829 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /size?dataset=conceptual_captions&config=labeled HTTP/1.1\" 200 None\n",
      "2024-03-03 17:47:08,833 - DEBUG - Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-03-03 17:47:09,238 - DEBUG - https://datasets-server.huggingface.co:443 \"GET /is-valid?dataset=poloclub/diffusiondb&config=2m_random_1m HTTP/1.1\" 500 None\n",
      "2024-03-03 17:47:12,104 - DEBUG - https://huggingface.co:443 \"HEAD /openai/clip-vit-large-patch14-336/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2024-03-03 17:47:12,411 - DEBUG - https://huggingface.co:443 \"HEAD /openai/clip-vit-large-patch14-336/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "2024-03-03 17:47:17,941 - DEBUG - https://huggingface.co:443 \"HEAD /openai/clip-vit-large-patch14-336/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2024-03-03 17:47:18,164 - DEBUG - https://huggingface.co:443 \"HEAD /openai/clip-vit-large-patch14-336/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from CustomPALI3.SummaryChartDataset import SummaryChartDataset\n",
    "from CustomPALI3.CustomPALI3 import CustomPALI3Config,CustomPALI3\n",
    "from transformers import T5Tokenizer\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from pytorch_model_summary import summary\n",
    "import numpy as np\n",
    "import gc\n",
    "dataset_repo=[{'dataset':'timm/imagenet-12k-wds','config':'default','type':'vision-text'},\n",
    "                {'dataset':'wikimedia/wikipedia','config':'20231101.en','type':'text'},\n",
    "                {'dataset':'conceptual_captions','config':'labeled','type':'vision-text'},\n",
    "                {'dataset':'poloclub/diffusiondb','config':'2m_random_1m','type':'vision-text'},\n",
    "                ]\n",
    "def my_collate_fn(samples):\n",
    "    image_batch = []\n",
    "    input_batch = []\n",
    "    attn_mask_batch = []\n",
    "    \n",
    "    batch_size=len(samples)\n",
    "    \n",
    "    image_batch_ = []\n",
    "    input_batch_ = []\n",
    "    attn_mask_batch_ = []\n",
    "    for sample in samples:\n",
    "        image_batch_.extend(sample['image'])\n",
    "        input_batch_.extend(sample['input_ids'])\n",
    "        attn_mask_batch_.extend(sample['attn_mask'])\n",
    "    \n",
    "    total_b=len(image_batch_)//batch_size # 14 //4   3.xx 3  \n",
    "    total_b=total_b+1 if len(image_batch_)%batch_size!=0  else total_b\n",
    "    for i in range(total_b):\n",
    "        if (i+1)*batch_size<len(image_batch_):\n",
    "            image_batch.append(torch.stack(image_batch_[i*batch_size:(i+1)*batch_size]))\n",
    "            input_batch.append(torch.stack(input_batch_[i*batch_size:(i+1)*batch_size]))\n",
    "            attn_mask_batch.append(torch.stack(attn_mask_batch_[i*batch_size:(i+1)*batch_size]))\n",
    "        else:\n",
    "            image_batch.append(torch.stack(image_batch_[i*batch_size:]))\n",
    "            input_batch.append(torch.stack(input_batch_[i*batch_size:]))\n",
    "            attn_mask_batch.append(torch.stack(attn_mask_batch_[i*batch_size:]))\n",
    "\n",
    "    return {'image': image_batch, 'input_ids': input_batch,'attn_mask':attn_mask_batch}\n",
    "args={\n",
    "    'output_dir':'/Users/dongunyun/study/datascience/chart2text/PALI3/output',\n",
    "    'lr':1e-4,\n",
    "    'max_steps':1e4,\n",
    "    'valid_steps':1e2,\n",
    "    'num_epochs':100,\n",
    "    'batch_size':4,\n",
    "    'num_training_samples_per_epoch':10,\n",
    "    'max_epochs':100,\n",
    "    \"warmup_steps\":100,\n",
    "    'num_workers':1,\n",
    "    'num_nodes':1,\n",
    "    }\n",
    "\n",
    "tokenizer=T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "train_loader=DataLoader(SummaryChartDataset(dataset_repo,1024,tokenizer,'</s>','train'), batch_size=1, shuffle=True, num_workers=1,collate_fn=my_collate_fn)\n",
    "val_loader=DataLoader(SummaryChartDataset(dataset_repo,1024,tokenizer,'</s>','validation'), batch_size=1, shuffle=True, num_workers=1,collate_fn=my_collate_fn)\n",
    "config=CustomPALI3Config(version=1,model_name='test',\n",
    "                    dim=1024,enc_num_tokens=32100,enc_max_seq_len=1024,\n",
    "                    dec_num_tokens=32100,dec_max_seq_len=1024,enc_depth=12,enc_heads=16,dec_depth=12,dec_heads=16,seq_len=1024\n",
    "                    ,device='mps',vit_fix=False)\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device =torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "model=CustomPALI3(config)\n",
    "model=model.from_pretrained(\"/Users/dongunyun/study/datascience/chart2text/PALI3/output_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "img=Image.open('conceptual_captions.png').convert(\"RGB\").resize((336,336),Image.Resampling.BILINEAR )\n",
    "label='Explain this picture. '\n",
    "\n",
    "input_image_tensor = transforms.transforms.ToTensor()(img).squeeze(0)\n",
    "input_image_tensor = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(input_image_tensor).unsqueeze(0).to(device)\n",
    "outputs = tokenizer(label, \n",
    "                                    max_length=1024, \n",
    "                                    padding=\"max_length\", \n",
    "                                    truncation=True,\n",
    "                                    return_tensors=\"pt\",\n",
    "                                    return_length=True,\n",
    "                                    )['input_ids'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1024 but got size 32100 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/study/datascience/chart2text/PALI3/CustomPALI3/CustomPALI3.py:70\u001b[0m, in \u001b[0;36mCustomPALI3.generate\u001b[0;34m(self, *arg)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;241m*\u001b[39marg):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/study/datascience/chart2text/PALI3/CustomPALI3/CustomPALI3.py:122\u001b[0m, in \u001b[0;36mCustomPALI3_.generate\u001b[0;34m(self, image, prompt, mask, attn_mask)\u001b[0m\n\u001b[1;32m    120\u001b[0m visual_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit_model(image)\n\u001b[1;32m    121\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpali_model\u001b[38;5;241m.\u001b[39mencoder(prompt)\n\u001b[0;32m--> 122\u001b[0m img_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m seq_out_start \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    124\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpali_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    125\u001b[0m     img_embeds, seq_out_start, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, mask, attn_mask\n\u001b[1;32m    126\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1024 but got size 32100 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "model.generate(input_image_tensor,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer=T5Tokenizer.from_pretrained(\"google/flan-t5-base\", bos_token = '<s>',add_bos_token = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:303: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[32100, 136, 589, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<s> any thing </s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
